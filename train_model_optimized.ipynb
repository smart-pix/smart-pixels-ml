{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from qkeras import *\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval = 1e9\n",
    "minval = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac065e8-fcf2-44a0-8dee-e1e44e605137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataprep import *\n",
    "from dataloaders.OptimizedDataGenerator import OptimizedDataGenerator\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb5ddc-dcd1-4224-8741-b137c8d5c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable batch normalization in the model (True, False)\n",
    "batch_norm_enabled = False\n",
    "# True = use all of the time slices, False = use a subset of the timeslices\n",
    "timeslices_all_enable = False\n",
    "# True = load tfrecords, False = (re-)generated tfrecords\n",
    "load_from_tfrecords_enabled = False\n",
    "# True = load model from file, False = train it from scratch\n",
    "load_model_from_hdf5_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf7c493-9a26-484f-a2b8-48f2f5626dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_norm_enabled:\n",
    "    from models_bnorm import *\n",
    "else:\n",
    "    from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a4fab-9a72-4728-ac5d-09fbd6f6afd8",
   "metadata": {},
   "source": [
    "# Scaling Lists for Different Pixel Pitches:\n",
    "* 100x25x100 um:  [150.0, 37.5, 10.0, 1.22]\n",
    "* 50x25x100 um:   [75.0, 37.5, 10.0, 1.22]\n",
    "* 50x20x100 um:   [75.0, 30.0, 10.0, 1.22]\n",
    "* 50x15x100 um:   [75.0, 22.5, 10.0, 1.22]\n",
    "* 50x12.5x100 um: [75.0, 18.75, 10.0, 1.22]\n",
    "* 50x10x100 um:   [75.0, 15.0, 10.0, 1.22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea9f75-3b9f-420b-a9b2-7b139d499b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can define a JSON configuration file locally\n",
    "# {\n",
    "#    \"data_base_dir\": \"/data/dajiang/smartPixels\",\n",
    "#    \"tfrecords_base_dir\" : \"/data/dajiang/smartPixels\",\n",
    "#    \"model_base_dir\": \"/home/dajiang/smart-pixels-ml/weights\"\n",
    "# }\n",
    "config_file_path = 'config.json'\n",
    "\n",
    "# If the file does not exist, the notebook uses default values for those entries\n",
    "data_base_dir = \"/data/dajiang/smartPixels/dataset_2s\"\n",
    "tfrecords_base_dir = \"/data/dajiang/smartPixels/tfrecords\"\n",
    "npy_base_dir = \"/data/dajiang/smartPixels/npy\"\n",
    "model_base_dir = \"/home/dajiang/smart-pixels-ml/weights\"\n",
    "\n",
    "if os.path.exists(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_base_dir = data.get('data_base_dir')\n",
    "        tfrecords_base_dir = data.get('tfrecords_base_dir')\n",
    "        npy_base_dir = data.get('npy_base_dir')\n",
    "        model_base_dir = data.get('model_base_dir')\n",
    "    print(f\"Use config info from file: {data_base_dir}, {tfrecords_base_dir}, {npy_base_dir}, {model_base_dir}\")\n",
    "else:\n",
    "    print(f\"File does not exist. Use default config info: {data_base_dir}, {tfrecords_base_dir}, {npy_base_dir}, {model_base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a0184-55a3-4294-b0df-b3067d9bb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 1000\n",
    "val_batch_size = 1000\n",
    "train_file_size = 50\n",
    "val_file_size = 10\n",
    "\n",
    "# See: https://docs.google.com/document/d/1ZoqVyJOOAXhzt2egMWh3OoNJ6lWq5lNR6sjcYON4Vlo/edit?tab=t.0#heading=h.k6tyal7z5t5l\n",
    "dataset_name = \"dataset_2s\"\n",
    "# 50x12.5x100 micron pixel sensor => 13x21 pixel sensor array\n",
    "sensor_geometry_name = \"50x12P5x100\"\n",
    "# Either 20 or 2 timeslices\n",
    "timeslices_name = \"timeslices20\" if timeslices_all_enable else \"timeslices2\"\n",
    "timeslices_val = -1 if timeslices_all_enable else [0, 19]\n",
    "#\n",
    "batch_size_name = f\"bs{batch_size}\"\n",
    "\n",
    "# Input: parquets\n",
    "data_dir = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/recon3D/\"\n",
    "labels_dir = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/labels/\"\n",
    "\n",
    "# Output: tfrecords\n",
    "tfrecords_dir_train = f\"{tfrecords_base_dir}/tfrecords_{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}_train\"\n",
    "tfrecords_dir_val = f\"{tfrecords_base_dir}/tfrecords_{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}_val\"\n",
    "\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    data_directory_path = data_dir,\n",
    "    labels_directory_path = labels_dir,\n",
    "    is_directory_recursive = False,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    scaling_list = [75.0, 18.75, 10.0, 1.22],\n",
    "    input_shape = (2,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    files_from_end=True,\n",
    "    shuffle= True,\n",
    "\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train if load_from_tfrecords_enabled else None,\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = (timeslices_val),\n",
    "    max_workers = 1, # Don't make this too large (will use up all RAM)\n",
    "    seed = 10,\n",
    "    quantize = True # Quantization ON\n",
    ")\n",
    "\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    data_directory_path = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/recon3D/\",\n",
    "    labels_directory_path = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/labels/\",\n",
    "    is_directory_recursive = False,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    scaling_list = [75.0, 18.75, 10.0, 1.22],\n",
    "    input_shape = (2,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    files_from_end=True,\n",
    "    shuffle= True,\n",
    "\n",
    "    load_from_tfrecords_dir = tfrecords_dir_val if load_from_tfrecords_enabled else None,\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = (timeslices_val),\n",
    "    max_workers = 1, # Don't make this too large (will use up all RAM)\n",
    "    seed = 10,\n",
    "    quantize = True # Quantization ON\n",
    ")\n",
    "\n",
    "print(f\"input: {data_dir}\")\n",
    "print(f\"input: {labels_dir}\")\n",
    "print(f\"output: {tfrecords_dir_train}\")\n",
    "print(f\"output: {tfrecords_dir_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4679b-c3e7-4c4e-a25a-90cbc845d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array that contains the recon3D and labels information\n",
    "# X is the 20-timeslices or 2-timeslices recon3D data\n",
    "# y is the labels data ['x-midplane','y-midplane','cotAlpha','cotBeta']\n",
    "X_val_all = []\n",
    "y_val_all = []\n",
    "\n",
    "val_num_batches = validation_generator.__len__() # The total number of batches for the validation dataset\n",
    "#val_num_batches = 1\n",
    "\n",
    "for batch_num in range(val_num_batches): # Loop over all batches\n",
    "    X_val, y_val = training_generator.__getitem__(batch_num) \n",
    "    X_val_all.append(X_val.numpy())\n",
    "    y_val_all.append(y_val.numpy())\n",
    "\n",
    "X_val_all = np.concatenate(X_val_all, axis=0)\n",
    "y_val_all = np.concatenate(y_val_all, axis=0)\n",
    "\n",
    "os.makedirs(npy_base_dir, exist_ok=True)\n",
    "np.save(f\"{npy_base_dir}/X_{timeslices_name}_val.npy\", X_val_all)\n",
    "np.save(f\"{npy_base_dir}/y_{timeslices_name}_val.npy\", y_val_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd3960-c7f1-4d6c-b7e0-0c4c833797d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiles model\n",
    "model = CreateModel((13,21,2), n_filters=5, pool_size=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b8741-98fd-4e1e-bc29-7b9fcbb9b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_name = f\"{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}\" + (\"_bnorm\" if batch_norm_enabled else \"\")\n",
    "\n",
    "best_model_hdf5 = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}.hdf5\"\n",
    "best_model_keras = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}.keras\"\n",
    "best_model_weights_hdf5 = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}_weights.hdf5\"\n",
    "best_model_weights_keras = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}_weights.keras\"\n",
    "best_model_architecture_json = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}_architecture.json\"\n",
    "\n",
    "if not load_model_from_hdf5_enabled:\n",
    "    # training\n",
    "    es = EarlyStopping(\n",
    "        patience=50,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    checkpoint_base_dir = f\"{model_base_dir}/weights_7pitches/{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}\"  + (\"_bnorm\" if batch_norm_enabled else \"\") + \"-checkpoints\"\n",
    "\n",
    "    os.makedirs(checkpoint_base_dir, exist_ok=True)\n",
    "    checkpoint_filepath = checkpoint_base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'\n",
    "    mcp = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=False,\n",
    "    )\n",
    "\n",
    "    class ScalePrintingCallback(keras.callbacks.Callback):    \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            scale_layer = self.model.layers[-1]\n",
    "            print(\n",
    "                f\"scaling layer ({epoch}):\", \n",
    "                scale_layer.scale, \n",
    "                tf.math.softplus(scale_layer.scale)\n",
    "            )\n",
    "\n",
    "    print_scale = ScalePrintingCallback()\n",
    "\n",
    "    history = model.fit(x=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        callbacks=[mcp],\n",
    "                        epochs=100,\n",
    "                        shuffle=False, # shuffling now occurs within the data-loader\n",
    "                        verbose=1)\n",
    "\n",
    "    # Revert to best model\n",
    "    files = os.listdir(checkpoint_base_dir)\n",
    "    vlosses = [float(f.split(\"-v\")[1].split(\".hdf5\")[0]) for f in files]\n",
    "    bestfile = files[np.argmin(vlosses)]\n",
    "    model.load_weights(f\"{checkpoint_base_dir}/{bestfile}\")\n",
    "\n",
    "    # Save (best) model information to file\n",
    "    model.save(best_model_hdf5)\n",
    "    model.save(best_model_keras)\n",
    "    model.save_weights(best_model_weights_hdf5)\n",
    "    model.save_weights(best_model_weights_keras)\n",
    "    model_json = model.to_json()\n",
    "    with open(best_model_architecture_json, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "else:\n",
    "    co = {\"custom_loss\": custom_loss}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    # This overrides the previously compiled model\n",
    "    # TODO: load just weights\n",
    "    model = load_model(best_model_hdf5, custom_objects=co)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a73561-06d0-49b5-8de4-a2ebd625209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_validation_loss_png = f\"{model_base_dir}/weights_7pitches/training_validation_loss_{model_name}.png\"\n",
    "if load_model_from_hdf5_enabled:\n",
    "    from PIL import Image\n",
    "    img = Image.open(training_validation_loss_png)\n",
    "    img.show()  # Opens the image in the default viewer\n",
    "else: \n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(training_validation_loss_png)  # Save as PNG\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
