{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:03:33.442772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-07 01:03:33.442875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-07 01:03:33.480221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-07 01:03:33.637284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-07 01:03:44.532929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9\n",
    "\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08697ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/das214/SmartPix/Datagenerator_debug\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('SmartPix/data_generator')\n",
    "os.chdir('SmartPix/Datagenerator_debug')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0170239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataprep import *\n",
    "# from OptimizedDataGeneratorNew import OptimizedDataGenerator\n",
    "from loss import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a74352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptimizedDataGeneratorNew.py\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union, List, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from qkeras import quantized_bits\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# custom quantizer\n",
    "\n",
    "# @tf.function\n",
    "def QKeras_data_prep_quantizer(data, bits=4, int_bits=0, alpha=1):\n",
    "    \"\"\"\n",
    "    Applies QKeras quantization.\n",
    "    Args:\n",
    "        data (tf.Tensor): Input data (tf.Tensor).\n",
    "        bits (int): Number of bits for quantization.\n",
    "        int_bits (int): Number of integer bits.\n",
    "        alpha (float): (don't change)\n",
    "    Returns::\n",
    "        tf.Tensor: Quantized data (tf.Tensor).\n",
    "    \"\"\"\n",
    "    quantizer = quantized_bits(bits, int_bits, alpha=alpha)\n",
    "    return quantizer(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85fc8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, \n",
    "            dataset_base_dir: str = \"./\",\n",
    "            batch_size: int = 32,\n",
    "            optimize_batch_size: bool = False,\n",
    "            file_count = None,\n",
    "            labels_list: Union[List,str] = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "            to_standardize: bool = False,\n",
    "            input_shape: Tuple = (13,21),\n",
    "            transpose = None,\n",
    "            files_from_end = False,\n",
    "            shuffle=False,\n",
    "\n",
    "            # Added in Optimized datagenerators \n",
    "            load_from_tfrecords_dir: str = None,\n",
    "            tfrecords_dir: str = None,\n",
    "            use_time_stamps = -1,\n",
    "            seed: int = None,\n",
    "            quantize: bool = False,\n",
    "            max_workers: int = 1,\n",
    "            label_scale_pctl: float = 99,\n",
    "            norm_pos_pctl: float = 99.7,\n",
    "            norm_neg_pctl: float = 99.7,\n",
    "            tail_tol: float = 0.75,\n",
    "            **kwargs,\n",
    "            ):\n",
    "        super().__init__() \n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        if shuffle:\n",
    "            self.seed = seed if seed is not None else 13\n",
    "            self.rng = np.random.default_rng(seed = self.seed)\n",
    "        \n",
    "        # If data is already prepared load -> load that data and use\n",
    "        if load_from_tfrecords_dir is not None:\n",
    "            self.file_offsets = [None]\n",
    "            if not os.path.isdir(load_from_tfrecords_dir):\n",
    "                raise ValueError(f\"Directory {load_from_tfrecords_dir} does not exist.\")\n",
    "            else:\n",
    "                self.tfrecords_dir = load_from_tfrecords_dir\n",
    "        else:\n",
    "            n_time, height, width = input_shape\n",
    "            \n",
    "            if use_time_stamps == -1:\n",
    "                use_time_stamps = list(np.arange(0,20))\n",
    "            assert len(use_time_stamps) == n_time, f\"Expected {n_time} time steps, got {len(use_time_stamps)}\"\n",
    "    \n",
    "            len_xy = height * width\n",
    "            col_indices = [\n",
    "                np.arange(t * len_xy, (t + 1) * len_xy).astype(str)\n",
    "                for t in use_time_stamps\n",
    "            ]\n",
    "            self.recon_cols = np.concatenate(col_indices).tolist()\n",
    "    \n",
    "            self.max_workers = max_workers\n",
    "            self.label_scale_pctl = label_scale_pctl\n",
    "            self.norm_pos_pctl = norm_pos_pctl\n",
    "            self.norm_neg_pctl = norm_neg_pctl\n",
    "\n",
    "            \n",
    "            self.files = sorted(glob.glob(os.path.join(dataset_base_dir, \"part.*.parquet\"), recursive=False))\n",
    "    \n",
    "            if file_count != None:\n",
    "                if not files_from_end:\n",
    "                    self.files = self.files[:file_count]\n",
    "                else:\n",
    "                    self.files = self.files[-file_count:]\n",
    "    \n",
    "            self.file_offsets = [0]\n",
    "            self.dataset_mean = None\n",
    "            self.dataset_std = None\n",
    "            self.norm_factor_pos = None  \n",
    "            self.norm_factor_neg = None\n",
    "            self.labels_scale = None\n",
    "\n",
    "            self.labels_list = labels_list\n",
    "            self.input_shape = input_shape\n",
    "            self.transpose = transpose\n",
    "            self.to_standardize = to_standardize\n",
    "\n",
    "           \n",
    "\n",
    "            self.process_file_parallel()\n",
    "            \n",
    "            \n",
    "            if optimize_batch_size:\n",
    "                original_bs = batch_size\n",
    "                new_bs, residual = self.get_best_batch_size(self.file_offsets, original_bs)\n",
    "                \n",
    "                if new_bs != original_bs:\n",
    "                    print(f\"Batch size optimized from {original_bs} to {new_bs} \"\n",
    "                        f\"to minimize final batch (residual: {residual} rows).\")\n",
    "                \n",
    "                self.batch_size = new_bs\n",
    "            else:\n",
    "                self.batch_size = batch_size\n",
    "\n",
    "            self.batch_metadata = self.build_batch_metadata(\n",
    "                batch_size=self.batch_size, \n",
    "                file_offsets=self.file_offsets, \n",
    "                tail_tol=tail_tol\n",
    "            )\n",
    "    \n",
    "            self.current_file_index = None\n",
    "            self.current_dataframes = None\n",
    "    \n",
    "            if tfrecords_dir is None:\n",
    "                raise ValueError(f\"tfrecords_dir is None\")\n",
    "            utils.safe_remove_directory(tfrecords_dir)\n",
    "                \n",
    "            self.tfrecords_dir = tfrecords_dir    \n",
    "            os.makedirs(self.tfrecords_dir, exist_ok=True)\n",
    "            self.save_batches_sequentially()\n",
    "            del self.current_dataframes \n",
    "            \n",
    "        self.tfrecord_filenames = np.sort(np.array(tf.io.gfile.glob(os.path.join(self.tfrecords_dir, \"*.tfrecord\"))))\n",
    "        self.quantize = quantize\n",
    "        self.epoch_count = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "\n",
    "    def process_file_parallel(self):\n",
    "        file_infos = [(afile, \n",
    "                    self.recon_cols, self.labels_list, \n",
    "                    self.label_scale_pctl, self.norm_pos_pctl, self.norm_neg_pctl) \n",
    "                    for afile in self.files\n",
    "                    ]\n",
    "        results = []\n",
    "        with ProcessPoolExecutor(self.max_workers) as executor:\n",
    "            futures = [executor.submit(self._process_file_single, file_info) for file_info in file_infos]\n",
    "            for future in tqdm(as_completed(futures), total=len(file_infos), desc=\"Processing Files...\"):\n",
    "                results.append(future.result())\n",
    "\n",
    "        for amean, avariance, amin, amax, num_rows, labels_scale, pos_scale, neg_scale in results:\n",
    "            self.file_offsets.append(self.file_offsets[-1] + num_rows)\n",
    "\n",
    "            if self.dataset_mean is None:\n",
    "                self.dataset_max = amax\n",
    "                self.dataset_min = amin\n",
    "                self.dataset_mean = amean\n",
    "                self.dataset_std = avariance\n",
    "            else:\n",
    "                self.dataset_max = max(self.dataset_max, amax)\n",
    "                self.dataset_min = min(self.dataset_min, amin)\n",
    "                self.dataset_mean += amean\n",
    "                self.dataset_std += avariance\n",
    "            \n",
    "            if self.labels_scale is None:\n",
    "                self.labels_scale = labels_scale\n",
    "            else:\n",
    "                self.labels_scale = np.maximum(self.labels_scale, labels_scale)\n",
    "\n",
    "            self.norm_factor_pos = (pos_scale if self.norm_factor_pos is None\n",
    "                                    else max(self.norm_factor_pos, pos_scale))\n",
    "            self.norm_factor_neg = (neg_scale if self.norm_factor_neg is None\n",
    "                                    else max(self.norm_factor_neg, neg_scale))\n",
    "\n",
    "        self.dataset_mean = self.dataset_mean / len(self.files)\n",
    "        self.dataset_std = np.sqrt(self.dataset_std / len(self.files)) \n",
    "            \n",
    "        self.file_offsets = np.array(self.file_offsets)\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_file_single(file_info):\n",
    "        afile, recon_cols, labels_list, label_scale_pctl, norm_pos_pctl, norm_neg_pctl = file_info\n",
    "\n",
    "        df = pd.read_parquet(afile, columns=recon_cols + labels_list).reset_index(drop=True)\n",
    "        x = df[recon_cols].values\n",
    "\n",
    "        nonzeros = abs(x) > 0\n",
    "        x[nonzeros] = np.sign(x[nonzeros]) * np.log1p(abs(x[nonzeros])) / math.log(2)\n",
    "        amean, avariance = np.mean(x[nonzeros], keepdims=True), np.var(x[nonzeros], keepdims=True) + 1e-10\n",
    "        centered = np.zeros_like(x)\n",
    "        centered[nonzeros] = (x[nonzeros] - amean) / np.sqrt(avariance)\n",
    "        amin, amax = np.min(centered), np.max(centered)\n",
    "\n",
    "        pos_vals = np.abs(centered[centered  > 0])\n",
    "        neg_vals = np.abs(centered[centered  < 0])\n",
    "\n",
    "        pos_scale = (np.percentile(pos_vals, norm_pos_pctl)\n",
    "                    if pos_vals.size else 1.0)\n",
    "        neg_scale = (np.percentile(neg_vals, norm_neg_pctl)\n",
    "                    if neg_vals.size else 1.0)\n",
    "\n",
    "        len_adf = len(df)\n",
    "\n",
    "        labels_values = df[labels_list].values\n",
    "        labels_scale = np.percentile(np.abs(labels_values), label_scale_pctl, axis=0)\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "        \n",
    "        return amean, avariance, amin, amax, len_adf, labels_scale, pos_scale, neg_scale\n",
    "\n",
    "    def standardize(self, x):\n",
    "        \"\"\"\n",
    "        Applies the normalization configuration in-place to a batch of inputs.\n",
    "        `x` is changed in-place since the function is mainly used internally\n",
    "        to standardize images and feed them to your network.\n",
    "        Args:\n",
    "            x: Batch of inputs to be normalized.\n",
    "        Returns:\n",
    "            The inputs, normalized. \n",
    "        \"\"\"\n",
    "        out = (x - self.dataset_mean)/self.dataset_std\n",
    "        out[out > 0] = out[out > 0]/self.norm_factor_pos\n",
    "        out[out < 0] = out[out < 0]/self.norm_factor_neg\n",
    "        out = np.clip(out, self.dataset_min, self.dataset_max)\n",
    "        return out\n",
    "\n",
    "    def save_batches_sequentially(self):\n",
    "        num_batches = self.__len__()\n",
    "        errors_found = []\n",
    "        for i in tqdm(range(num_batches), desc=\"Saving batches as TFRecords\"):\n",
    "            result = self.save_single_batch(i)\n",
    "            if \"Error\" in result:\n",
    "                print(result)\n",
    "                errors_found.append(result)\n",
    "        \n",
    "        if errors_found:\n",
    "            logging.warning(f\"Encountered {len(errors_found)} errors during sequential saving of TFRecords.\")\n",
    "        else:\n",
    "            logging.info(\"All batches saved successfully in sequential mode.\")\n",
    "\n",
    "\n",
    "    def save_single_batch(self, batch_index):\n",
    "        \"\"\"\n",
    "        Serializes and saves a single batch to a TFRecord file.\n",
    "        Args:\n",
    "            batch_index (int): Index of the batch to save.\n",
    "        Returns:\n",
    "            str: Path to the saved TFRecord file or an error message.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            filename = f\"batch_{batch_index}.tfrecord\"\n",
    "            TFRfile_path = os.path.join(self.tfrecords_dir, filename)\n",
    "            X, y = self.prepare_batch_data(batch_index)\n",
    "            serialized_example = self.serialize_example(X, y)\n",
    "            with tf.io.TFRecordWriter(TFRfile_path) as writer:\n",
    "                writer.write(serialized_example)\n",
    "            return TFRfile_path\n",
    "        except Exception as e:\n",
    "            return f\"Error saving batch {batch_index}: {e}\" \n",
    "      \n",
    "    @staticmethod  \n",
    "    def get_best_batch_size(file_offsets, target_bs=5000):\n",
    "        \"\"\"\n",
    "        Find the best batch size that minimizes the residual when dividing the total number of rows.\n",
    "        Args:\n",
    "            file_offsets (np.ndarray): Array of file offsets.\n",
    "            target_bs (int): Target batch size.\n",
    "            tol (float): Tolerance for batch size deviation.\n",
    "        Returns:\n",
    "            int: Best batch size.\n",
    "        \"\"\"\n",
    "        last_offset = file_offsets[-1]\n",
    "        d_bs = int(0.5 * target_bs)\n",
    "        batch_sizes = np.arange(target_bs - d_bs, target_bs + d_bs + 1)\n",
    "\n",
    "        residuals = last_offset % batch_sizes\n",
    "        min_res   = residuals.min()\n",
    "\n",
    "        # All bs giving the minimal residual\n",
    "        candidates = batch_sizes[residuals == min_res]\n",
    "\n",
    "        # Prefer the one closest to the target\n",
    "        idx = np.argmin(np.abs(candidates - target_bs))\n",
    "        return int(candidates[idx]), min_res    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_batching_plan(file_offsets, batch_size, tol = 0.75):\n",
    "        \"\"\"\n",
    "        Pre-compute (row_start, row_end) for every batch.\n",
    "        If the last batch < 0.5xbatch_size, merge the last two\n",
    "        and split them evenly, so both new batches are within\n",
    "        0.5x...1.0xbatch_size.\n",
    "        \"\"\"\n",
    "        total = file_offsets[-1]\n",
    "        b      = batch_size\n",
    "        plan   = []\n",
    "        start  = 0\n",
    "        while start < total:\n",
    "            end = min(start + b, total)\n",
    "            plan.append((start, end))\n",
    "            start = end\n",
    "\n",
    "        # Re-balance if the tail is too short\n",
    "        if len(plan) >= 2:\n",
    "            last_len = plan[-1][1] - plan[-1][0]\n",
    "            if last_len < tol * b:\n",
    "                sec_start = plan[-2][0]\n",
    "                comb_len  = plan[-1][1] - sec_start\n",
    "                half      = math.ceil(comb_len / 2)\n",
    "                plan[-2]  = (sec_start, sec_start + half)\n",
    "                plan[-1]  = (sec_start + half, sec_start + comb_len)\n",
    "        return plan\n",
    "    \n",
    "    @classmethod\n",
    "    def build_batch_metadata(cls, batch_size: int, file_offsets: np.ndarray, tail_tol: float = 0.75) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Builds optimized batch metadata using a pre-computed batch plan.\n",
    "        This ensures that the final batch is not excessively small.\n",
    "        \"\"\"\n",
    "        batching_plan = cls._build_batching_plan(file_offsets, batch_size, tail_tol)\n",
    "        batch_metadata = []\n",
    "\n",
    "        # 2. Loop through the generated plan instead of a simple range\n",
    "        for batch_index, (start_evt, end_evt) in enumerate(batching_plan):\n",
    "            \n",
    "            # Create a new dictionary for the current batch\n",
    "            current_batch_meta = {\n",
    "                \"batch_idx\": batch_index,\n",
    "                \"target_batch_size\": int(batch_size),\n",
    "                # The actual size is now simply the difference from the plan\n",
    "                \"actual_batch_size\": int(end_evt - start_evt),\n",
    "                \"segments\": []\n",
    "            }\n",
    "\n",
    "            # 3. Use the same logic as before to find the file segments for the given range\n",
    "            file_idx = np.searchsorted(file_offsets, start_evt, side=\"right\") - 1\n",
    "            evt_cursor = start_evt\n",
    "\n",
    "            while evt_cursor < end_evt:\n",
    "                file_start = file_offsets[file_idx]\n",
    "                file_end = file_offsets[file_idx + 1]\n",
    "\n",
    "                rel_start = evt_cursor - file_start\n",
    "                rel_end = min(end_evt, file_end) - file_start\n",
    "                \n",
    "                # Append segment info to the current batch's metadata\n",
    "                current_batch_meta[\"segments\"].append({\n",
    "                    \"file_idx\": int(file_idx),\n",
    "                    \"row_start\": int(rel_start),\n",
    "                    \"row_end\": int(rel_end - 1)\n",
    "                })\n",
    "                \n",
    "                evt_cursor += (rel_end - rel_start)\n",
    "                file_idx += 1\n",
    "            \n",
    "            batch_metadata.append(current_batch_meta)\n",
    "\n",
    "        return batch_metadata\n",
    " \n",
    "    def prepare_batch_data(self, batch_index):\n",
    "        batch_plan = self.batch_metadata[batch_index]\n",
    "\n",
    "        X_chunks = []\n",
    "        y_chunks = []\n",
    "\n",
    "        for segment in batch_plan[\"segments\"]:\n",
    "            file_idx = segment[\"file_idx\"]\n",
    "            rel_start = segment[\"row_start\"]\n",
    "            rel_end = segment[\"row_end\"] + 1  # inclusive end\n",
    "\n",
    "            if file_idx != self.current_file_index:\n",
    "                parquet_file = self.files[file_idx]\n",
    "                df = (pd.read_parquet(parquet_file,\n",
    "                                    columns=self.recon_cols + self.labels_list)\n",
    "                        .dropna(subset=self.recon_cols)\n",
    "                        .reset_index(drop=True))\n",
    "                if self.shuffle:\n",
    "                    df = df.sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
    "                recon_df  = df[self.recon_cols]\n",
    "                labels_df = df[self.labels_list]\n",
    "\n",
    "                recon_values = recon_df.values\n",
    "                nonzeros = abs(recon_values) > 0\n",
    "                recon_values[nonzeros] = np.sign(recon_values[nonzeros]) * np.log1p(abs(recon_values[nonzeros])) / np.log(2)\n",
    "                if self.to_standardize:\n",
    "                    recon_values[nonzeros] = self.standardize(recon_values[nonzeros])\n",
    "                recon_values = recon_values.reshape((-1, *self.input_shape))\n",
    "                if self.transpose is not None:\n",
    "                    recon_values = recon_values.transpose(self.transpose)\n",
    "                self.current_dataframes = (\n",
    "                    recon_values, \n",
    "                    labels_df.values,\n",
    "                )\n",
    "                self.current_file_index = file_idx\n",
    "                del df\n",
    "                gc.collect()\n",
    "\n",
    "            recon_df, labels_df = self.current_dataframes\n",
    "            X_chunk = recon_df[rel_start:rel_end]\n",
    "            y_chunk = labels_df[rel_start:rel_end] / self.labels_scale\n",
    "\n",
    "            X_chunks.append(X_chunk)\n",
    "            y_chunks.append(y_chunk)\n",
    "\n",
    "\n",
    "\n",
    "        X = np.concatenate(X_chunks, axis=0)\n",
    "        y = np.concatenate(y_chunks, axis=0)\n",
    "\n",
    "        return X, y\n",
    "   \n",
    "\n",
    "    def serialize_example(self, X, y):\n",
    "        \"\"\"\n",
    "        Serializes a single example (featuresand labels) to TFRecord format. \n",
    "        \n",
    "        Args:\n",
    "        - X: Training data\n",
    "        - y: labelled data\n",
    "        \n",
    "        Returns:\n",
    "        - string (serialized TFRecord example).\n",
    "        \"\"\"\n",
    "        # X and y are float32 (maybe we can reduce this)\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "\n",
    "        feature = {\n",
    "            'X': self._bytes_feature(tf.io.serialize_tensor(X)),\n",
    "            'y': self._bytes_feature(tf.io.serialize_tensor(y)),\n",
    "        }\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    @staticmethod\n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"\n",
    "        Converts a string/byte value into a Tf feature of bytes_list\n",
    "        \n",
    "        Args: \n",
    "        - string/byte value\n",
    "        \n",
    "        Returns:\n",
    "        - tf.train.Feature object as a bytes_list containing the input value.\n",
    "        \"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))): # check if Tf tensor\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "        \"\"\"\n",
    "        Load the batch from a pre-saved TFRecord file instead of processing raw data.\n",
    "        Each file contains exactly one batch.\n",
    "        quantization is done here: Helpful for pretraining without the quantization and the later training with quantized data.\n",
    "        shuffling is also done here.\n",
    "        TODO: prefetching (un-done)\n",
    "        \"\"\"\n",
    "        tfrecord_path = self.tfrecord_filenames[batch_index]\n",
    "        raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "        parsed_dataset = raw_dataset.map(self._parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        # Get the first (and only) batch from the dataset\n",
    "        try:\n",
    "            X_batch, y_batch = next(iter(parsed_dataset))\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"No data found in TFRecord file: {tfrecord_path}\")\n",
    "\n",
    "        X_batch = tf.reshape(X_batch, [-1, *X_batch.shape[1:]])\n",
    "        y_batch = tf.reshape(y_batch, [-1, *y_batch.shape[1:]])\n",
    "\n",
    "        if self.quantize:\n",
    "            X_batch = QKeras_data_prep_quantizer(X_batch, bits=4, int_bits=0, alpha=1)\n",
    "\n",
    "        if self.shuffle:\n",
    "            indices = tf.range(start=0, limit=tf.shape(X_batch)[0], dtype=tf.int32)\n",
    "            shuffled_indices = tf.random.shuffle(indices, seed=self.seed)\n",
    "            X_batch = tf.gather(X_batch, shuffled_indices)\n",
    "            y_batch = tf.gather(y_batch, shuffled_indices)\n",
    "\n",
    "        del raw_dataset, parsed_dataset\n",
    "        return X_batch, y_batch\n",
    "            \n",
    "    @staticmethod\n",
    "    def _parse_tfrecord_fn(example):\n",
    "        \"\"\"\n",
    "        Parses a single TFRecord example.\n",
    "        \n",
    "        Returns:\n",
    "        - X: as a float32 tensor.\n",
    "        - y: as a float32 tensor.\n",
    "        \"\"\"\n",
    "        feature_description = {\n",
    "            'X': tf.io.FixedLenFeature([], tf.string),\n",
    "            'y': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        X = tf.io.parse_tensor(example['X'], out_type=tf.float32)\n",
    "        y = tf.io.parse_tensor(example['y'], out_type=tf.float32)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Phase-aware length:\n",
    "            during initial TFRecord creation: math on file_offsets\n",
    "            after creation in same process: len(batch_metadata)\n",
    "            when loading existing TFRecords: len(tfrecord_filenames)\n",
    "        \"\"\"\n",
    "        # already have metadata?  Fastest answer.\n",
    "        if self.batch_metadata:\n",
    "            return len(self.batch_metadata)\n",
    "\n",
    "        # still building batches, so compute from source rows.\n",
    "        if len(self.file_offsets) > 1:         # have real offsets\n",
    "            total_rows = self.file_offsets[-1]\n",
    "            return math.ceil(total_rows / self.batch_size)\n",
    "\n",
    "        # running in \"load\" mode.\n",
    "        self.tfrecord_filenames = np.sort(\n",
    "            np.array(tf.io.gfile.glob(\n",
    "                os.path.join(self.tfrecords_dir, \"*.tfrecord\"))))\n",
    "        return len(self.tfrecord_filenames)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        This shuffles the file ordering so that it shuffles the ordering in which the TFRecord\n",
    "        are loaded during the training for each epochs.\n",
    "        '''\n",
    "        gc.collect()\n",
    "        self.epoch_count += 1\n",
    "        # Log quantization status once\n",
    "        if self.epoch_count == 1:\n",
    "            logging.warning(f\"Quantization is {self.quantize} in data generator. This may affect model performance.\")\n",
    "\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(self.tfrecord_filenames)\n",
    "            self.seed += 1 # So that after each epoch the batch is shuffled with a different seed (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd94db45-10a3-4aa0-9885-cad5c6cc25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/\"\n",
    "tfrecords_base_dir = os.path.join(dataset_base_dir, \"TFR_files\", \"2t\")\n",
    "\n",
    "dataset_base_dir = os.path.join(dataset_base_dir, \"parquets\")\n",
    "tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\")\n",
    "tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\")\n",
    "\n",
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = 75\n",
    "val_file_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccba057-7aa7-4b85-95bd-8977c14cf7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...:   0%|          | 0/25 [00:06<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    # optimize_batch_size = True,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, \n",
    "    files_from_end=True,\n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2\n",
    ")\n",
    "\n",
    "print(\"--- Validation generator %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# training generator\n",
    "start_time = time.time()\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    # optimize_batch_size = True,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, # True \n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2\n",
    ")\n",
    "print(\"--- Training generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb2dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,   20374,   40748,   61122,   81496,  101870,  122244,\n",
       "        142618,  162992,  183366,  203740,  224114,  244488,  264862,\n",
       "        285236,  305610,  325984,  346358,  366732,  387106,  407480,\n",
       "        427854,  448228,  468602,  488976,  509350,  529724,  550098,\n",
       "        570472,  590846,  611220,  631594,  651968,  672342,  692716,\n",
       "        713090,  733464,  753838,  774212,  794586,  814960,  835334,\n",
       "        855708,  876082,  896456,  916830,  937204,  957578,  977952,\n",
       "        998326, 1018700, 1039074, 1059448, 1079822, 1100196, 1120570,\n",
       "       1140944, 1161318, 1181692, 1202066, 1222440, 1242814, 1263188,\n",
       "       1283562, 1303936, 1324310, 1344684, 1365058, 1385432, 1405806,\n",
       "       1426180, 1446554, 1466928, 1487302, 1507676, 1528050])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.file_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dc1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking auto normalization factors:\n",
      "Training generator norm factor pos: 1.4030861830865065\n",
      "Training generator norm factor neg: 2.4879350274661034\n",
      "\n",
      "Checking auto scale factors:\n",
      "Training generator labels scale: [74.36465612 18.59104573  8.60158289  0.53649678]\n",
      "Training generator dataset mean: [5.02385156]\n",
      "Training generator dataset std: [6.11337844]\n",
      "Training generator dataset max: 1.9337638243125104\n",
      "Training generator dataset min: -2.8022317516663517\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking auto normalization factors:\")\n",
    "print(\"Training generator norm factor pos:\", training_generator.norm_factor_pos)\n",
    "print(\"Training generator norm factor neg:\", training_generator.norm_factor_neg)\n",
    "print()\n",
    "print(\"Checking auto scale factors:\")\n",
    "print(\"Training generator labels scale:\", training_generator.labels_scale)\n",
    "print(\"Training generator dataset mean:\", training_generator.dataset_mean)\n",
    "print(\"Training generator dataset std:\", training_generator.dataset_std)\n",
    "print(\"Training generator dataset max:\", training_generator.dataset_max)\n",
    "print(\"Training generator dataset min:\", training_generator.dataset_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077183f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batch_idx': 0,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 0, 'row_end': 5009}]},\n",
       " {'batch_idx': 1,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 5010, 'row_end': 10019}]},\n",
       " {'batch_idx': 2,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 10020, 'row_end': 15029}]},\n",
       " {'batch_idx': 3,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 15030, 'row_end': 20039}]},\n",
       " {'batch_idx': 4,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 20040, 'row_end': 20373},\n",
       "   {'file_idx': 1, 'row_start': 0, 'row_end': 4675}]},\n",
       " {'batch_idx': 5,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 4676, 'row_end': 9685}]},\n",
       " {'batch_idx': 6,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 9686, 'row_end': 14695}]},\n",
       " {'batch_idx': 7,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 14696, 'row_end': 19705}]},\n",
       " {'batch_idx': 8,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 19706, 'row_end': 20373},\n",
       "   {'file_idx': 2, 'row_start': 0, 'row_end': 4341}]},\n",
       " {'batch_idx': 9,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 4342, 'row_end': 9351}]},\n",
       " {'batch_idx': 10,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 9352, 'row_end': 14361}]},\n",
       " {'batch_idx': 11,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 14362, 'row_end': 19371}]},\n",
       " {'batch_idx': 12,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 19372, 'row_end': 20373},\n",
       "   {'file_idx': 3, 'row_start': 0, 'row_end': 4007}]},\n",
       " {'batch_idx': 13,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 4008, 'row_end': 9017}]},\n",
       " {'batch_idx': 14,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 9018, 'row_end': 14027}]},\n",
       " {'batch_idx': 15,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 14028, 'row_end': 19037}]},\n",
       " {'batch_idx': 16,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 19038, 'row_end': 20373},\n",
       "   {'file_idx': 4, 'row_start': 0, 'row_end': 3673}]},\n",
       " {'batch_idx': 17,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 3674, 'row_end': 8683}]},\n",
       " {'batch_idx': 18,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 8684, 'row_end': 13693}]},\n",
       " {'batch_idx': 19,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 13694, 'row_end': 18703}]},\n",
       " {'batch_idx': 20,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 18704, 'row_end': 20373},\n",
       "   {'file_idx': 5, 'row_start': 0, 'row_end': 3339}]},\n",
       " {'batch_idx': 21,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 3340, 'row_end': 8349}]},\n",
       " {'batch_idx': 22,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 8350, 'row_end': 13359}]},\n",
       " {'batch_idx': 23,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 13360, 'row_end': 18369}]},\n",
       " {'batch_idx': 24,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 18370, 'row_end': 20373},\n",
       "   {'file_idx': 6, 'row_start': 0, 'row_end': 3005}]},\n",
       " {'batch_idx': 25,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 3006, 'row_end': 8015}]},\n",
       " {'batch_idx': 26,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 8016, 'row_end': 13025}]},\n",
       " {'batch_idx': 27,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 13026, 'row_end': 18035}]},\n",
       " {'batch_idx': 28,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 18036, 'row_end': 20373},\n",
       "   {'file_idx': 7, 'row_start': 0, 'row_end': 2671}]},\n",
       " {'batch_idx': 29,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 2672, 'row_end': 7681}]},\n",
       " {'batch_idx': 30,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 7682, 'row_end': 12691}]},\n",
       " {'batch_idx': 31,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 12692, 'row_end': 17701}]},\n",
       " {'batch_idx': 32,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 17702, 'row_end': 20373},\n",
       "   {'file_idx': 8, 'row_start': 0, 'row_end': 2337}]},\n",
       " {'batch_idx': 33,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 2338, 'row_end': 7347}]},\n",
       " {'batch_idx': 34,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 7348, 'row_end': 12357}]},\n",
       " {'batch_idx': 35,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 12358, 'row_end': 17367}]},\n",
       " {'batch_idx': 36,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 17368, 'row_end': 20373},\n",
       "   {'file_idx': 9, 'row_start': 0, 'row_end': 2003}]},\n",
       " {'batch_idx': 37,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 2004, 'row_end': 7013}]},\n",
       " {'batch_idx': 38,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 7014, 'row_end': 12023}]},\n",
       " {'batch_idx': 39,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 12024, 'row_end': 17033}]},\n",
       " {'batch_idx': 40,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 17034, 'row_end': 20373},\n",
       "   {'file_idx': 10, 'row_start': 0, 'row_end': 1669}]},\n",
       " {'batch_idx': 41,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 1670, 'row_end': 6679}]},\n",
       " {'batch_idx': 42,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 6680, 'row_end': 11689}]},\n",
       " {'batch_idx': 43,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 11690, 'row_end': 16699}]},\n",
       " {'batch_idx': 44,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 16700, 'row_end': 20373},\n",
       "   {'file_idx': 11, 'row_start': 0, 'row_end': 1335}]},\n",
       " {'batch_idx': 45,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 1336, 'row_end': 6345}]},\n",
       " {'batch_idx': 46,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 6346, 'row_end': 11355}]},\n",
       " {'batch_idx': 47,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 11356, 'row_end': 16365}]},\n",
       " {'batch_idx': 48,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 16366, 'row_end': 20373},\n",
       "   {'file_idx': 12, 'row_start': 0, 'row_end': 1001}]},\n",
       " {'batch_idx': 49,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 1002, 'row_end': 6011}]},\n",
       " {'batch_idx': 50,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 6012, 'row_end': 11021}]},\n",
       " {'batch_idx': 51,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 11022, 'row_end': 16031}]},\n",
       " {'batch_idx': 52,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 16032, 'row_end': 20373},\n",
       "   {'file_idx': 13, 'row_start': 0, 'row_end': 667}]},\n",
       " {'batch_idx': 53,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 668, 'row_end': 5677}]},\n",
       " {'batch_idx': 54,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 5678, 'row_end': 10687}]},\n",
       " {'batch_idx': 55,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 10688, 'row_end': 15697}]},\n",
       " {'batch_idx': 56,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 15698, 'row_end': 20373},\n",
       "   {'file_idx': 14, 'row_start': 0, 'row_end': 333}]},\n",
       " {'batch_idx': 57,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 334, 'row_end': 5343}]},\n",
       " {'batch_idx': 58,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 5344, 'row_end': 10353}]},\n",
       " {'batch_idx': 59,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 10354, 'row_end': 15363}]},\n",
       " {'batch_idx': 60,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 15364, 'row_end': 20373}]},\n",
       " {'batch_idx': 61,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 0, 'row_end': 5009}]},\n",
       " {'batch_idx': 62,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 5010, 'row_end': 10019}]},\n",
       " {'batch_idx': 63,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 10020, 'row_end': 15029}]},\n",
       " {'batch_idx': 64,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 15030, 'row_end': 20039}]},\n",
       " {'batch_idx': 65,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 20040, 'row_end': 20373},\n",
       "   {'file_idx': 16, 'row_start': 0, 'row_end': 4675}]},\n",
       " {'batch_idx': 66,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 4676, 'row_end': 9685}]},\n",
       " {'batch_idx': 67,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 9686, 'row_end': 14695}]},\n",
       " {'batch_idx': 68,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 14696, 'row_end': 19705}]},\n",
       " {'batch_idx': 69,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 19706, 'row_end': 20373},\n",
       "   {'file_idx': 17, 'row_start': 0, 'row_end': 4341}]},\n",
       " {'batch_idx': 70,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 4342, 'row_end': 9351}]},\n",
       " {'batch_idx': 71,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 9352, 'row_end': 14361}]},\n",
       " {'batch_idx': 72,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 14362, 'row_end': 19371}]},\n",
       " {'batch_idx': 73,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 19372, 'row_end': 20373},\n",
       "   {'file_idx': 18, 'row_start': 0, 'row_end': 4007}]},\n",
       " {'batch_idx': 74,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 4008, 'row_end': 9017}]},\n",
       " {'batch_idx': 75,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 9018, 'row_end': 14027}]},\n",
       " {'batch_idx': 76,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 14028, 'row_end': 19037}]},\n",
       " {'batch_idx': 77,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 19038, 'row_end': 20373},\n",
       "   {'file_idx': 19, 'row_start': 0, 'row_end': 3673}]},\n",
       " {'batch_idx': 78,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 3674, 'row_end': 8683}]},\n",
       " {'batch_idx': 79,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 8684, 'row_end': 13693}]},\n",
       " {'batch_idx': 80,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 13694, 'row_end': 18703}]},\n",
       " {'batch_idx': 81,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 18704, 'row_end': 20373},\n",
       "   {'file_idx': 20, 'row_start': 0, 'row_end': 3339}]},\n",
       " {'batch_idx': 82,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 3340, 'row_end': 8349}]},\n",
       " {'batch_idx': 83,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 8350, 'row_end': 13359}]},\n",
       " {'batch_idx': 84,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 13360, 'row_end': 18369}]},\n",
       " {'batch_idx': 85,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 18370, 'row_end': 20373},\n",
       "   {'file_idx': 21, 'row_start': 0, 'row_end': 3005}]},\n",
       " {'batch_idx': 86,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 3006, 'row_end': 8015}]},\n",
       " {'batch_idx': 87,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 8016, 'row_end': 13025}]},\n",
       " {'batch_idx': 88,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 13026, 'row_end': 18035}]},\n",
       " {'batch_idx': 89,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 18036, 'row_end': 20373},\n",
       "   {'file_idx': 22, 'row_start': 0, 'row_end': 2671}]},\n",
       " {'batch_idx': 90,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 2672, 'row_end': 7681}]},\n",
       " {'batch_idx': 91,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 7682, 'row_end': 12691}]},\n",
       " {'batch_idx': 92,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 12692, 'row_end': 17701}]},\n",
       " {'batch_idx': 93,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 17702, 'row_end': 20373},\n",
       "   {'file_idx': 23, 'row_start': 0, 'row_end': 2337}]},\n",
       " {'batch_idx': 94,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 2338, 'row_end': 7347}]},\n",
       " {'batch_idx': 95,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 7348, 'row_end': 12357}]},\n",
       " {'batch_idx': 96,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 12358, 'row_end': 17367}]},\n",
       " {'batch_idx': 97,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 17368, 'row_end': 20373},\n",
       "   {'file_idx': 24, 'row_start': 0, 'row_end': 2003}]},\n",
       " {'batch_idx': 98,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 2004, 'row_end': 7013}]},\n",
       " {'batch_idx': 99,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 7014, 'row_end': 12023}]},\n",
       " {'batch_idx': 100,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 12024, 'row_end': 17033}]},\n",
       " {'batch_idx': 101,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 17034, 'row_end': 20373},\n",
       "   {'file_idx': 25, 'row_start': 0, 'row_end': 1669}]},\n",
       " {'batch_idx': 102,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 1670, 'row_end': 6679}]},\n",
       " {'batch_idx': 103,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 6680, 'row_end': 11689}]},\n",
       " {'batch_idx': 104,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 11690, 'row_end': 16699}]},\n",
       " {'batch_idx': 105,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 16700, 'row_end': 20373},\n",
       "   {'file_idx': 26, 'row_start': 0, 'row_end': 1335}]},\n",
       " {'batch_idx': 106,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 1336, 'row_end': 6345}]},\n",
       " {'batch_idx': 107,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 6346, 'row_end': 11355}]},\n",
       " {'batch_idx': 108,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 11356, 'row_end': 16365}]},\n",
       " {'batch_idx': 109,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 16366, 'row_end': 20373},\n",
       "   {'file_idx': 27, 'row_start': 0, 'row_end': 1001}]},\n",
       " {'batch_idx': 110,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 1002, 'row_end': 6011}]},\n",
       " {'batch_idx': 111,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 6012, 'row_end': 11021}]},\n",
       " {'batch_idx': 112,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 11022, 'row_end': 16031}]},\n",
       " {'batch_idx': 113,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 16032, 'row_end': 20373},\n",
       "   {'file_idx': 28, 'row_start': 0, 'row_end': 667}]},\n",
       " {'batch_idx': 114,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 668, 'row_end': 5677}]},\n",
       " {'batch_idx': 115,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 5678, 'row_end': 10687}]},\n",
       " {'batch_idx': 116,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 10688, 'row_end': 15697}]},\n",
       " {'batch_idx': 117,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 15698, 'row_end': 20373},\n",
       "   {'file_idx': 29, 'row_start': 0, 'row_end': 333}]},\n",
       " {'batch_idx': 118,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 334, 'row_end': 5343}]},\n",
       " {'batch_idx': 119,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 5344, 'row_end': 10353}]},\n",
       " {'batch_idx': 120,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 10354, 'row_end': 15363}]},\n",
       " {'batch_idx': 121,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 15364, 'row_end': 20373}]},\n",
       " {'batch_idx': 122,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 0, 'row_end': 5009}]},\n",
       " {'batch_idx': 123,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 5010, 'row_end': 10019}]},\n",
       " {'batch_idx': 124,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 10020, 'row_end': 15029}]},\n",
       " {'batch_idx': 125,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 15030, 'row_end': 20039}]},\n",
       " {'batch_idx': 126,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 20040, 'row_end': 20373},\n",
       "   {'file_idx': 31, 'row_start': 0, 'row_end': 4675}]},\n",
       " {'batch_idx': 127,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 4676, 'row_end': 9685}]},\n",
       " {'batch_idx': 128,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 9686, 'row_end': 14695}]},\n",
       " {'batch_idx': 129,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 14696, 'row_end': 19705}]},\n",
       " {'batch_idx': 130,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 19706, 'row_end': 20373},\n",
       "   {'file_idx': 32, 'row_start': 0, 'row_end': 4341}]},\n",
       " {'batch_idx': 131,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 4342, 'row_end': 9351}]},\n",
       " {'batch_idx': 132,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 9352, 'row_end': 14361}]},\n",
       " {'batch_idx': 133,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 14362, 'row_end': 19371}]},\n",
       " {'batch_idx': 134,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 19372, 'row_end': 20373},\n",
       "   {'file_idx': 33, 'row_start': 0, 'row_end': 4007}]},\n",
       " {'batch_idx': 135,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 4008, 'row_end': 9017}]},\n",
       " {'batch_idx': 136,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 9018, 'row_end': 14027}]},\n",
       " {'batch_idx': 137,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 14028, 'row_end': 19037}]},\n",
       " {'batch_idx': 138,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 19038, 'row_end': 20373},\n",
       "   {'file_idx': 34, 'row_start': 0, 'row_end': 3673}]},\n",
       " {'batch_idx': 139,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 3674, 'row_end': 8683}]},\n",
       " {'batch_idx': 140,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 8684, 'row_end': 13693}]},\n",
       " {'batch_idx': 141,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 13694, 'row_end': 18703}]},\n",
       " {'batch_idx': 142,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 18704, 'row_end': 20373},\n",
       "   {'file_idx': 35, 'row_start': 0, 'row_end': 3339}]},\n",
       " {'batch_idx': 143,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 3340, 'row_end': 8349}]},\n",
       " {'batch_idx': 144,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 8350, 'row_end': 13359}]},\n",
       " {'batch_idx': 145,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 13360, 'row_end': 18369}]},\n",
       " {'batch_idx': 146,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 18370, 'row_end': 20373},\n",
       "   {'file_idx': 36, 'row_start': 0, 'row_end': 3005}]},\n",
       " {'batch_idx': 147,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 3006, 'row_end': 8015}]},\n",
       " {'batch_idx': 148,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 8016, 'row_end': 13025}]},\n",
       " {'batch_idx': 149,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 13026, 'row_end': 18035}]},\n",
       " {'batch_idx': 150,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 18036, 'row_end': 20373},\n",
       "   {'file_idx': 37, 'row_start': 0, 'row_end': 2671}]},\n",
       " {'batch_idx': 151,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 2672, 'row_end': 7681}]},\n",
       " {'batch_idx': 152,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 7682, 'row_end': 12691}]},\n",
       " {'batch_idx': 153,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 12692, 'row_end': 17701}]},\n",
       " {'batch_idx': 154,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 17702, 'row_end': 20373},\n",
       "   {'file_idx': 38, 'row_start': 0, 'row_end': 2337}]},\n",
       " {'batch_idx': 155,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 2338, 'row_end': 7347}]},\n",
       " {'batch_idx': 156,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 7348, 'row_end': 12357}]},\n",
       " {'batch_idx': 157,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 12358, 'row_end': 17367}]},\n",
       " {'batch_idx': 158,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 17368, 'row_end': 20373},\n",
       "   {'file_idx': 39, 'row_start': 0, 'row_end': 2003}]},\n",
       " {'batch_idx': 159,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 2004, 'row_end': 7013}]},\n",
       " {'batch_idx': 160,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 7014, 'row_end': 12023}]},\n",
       " {'batch_idx': 161,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 12024, 'row_end': 17033}]},\n",
       " {'batch_idx': 162,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 17034, 'row_end': 20373},\n",
       "   {'file_idx': 40, 'row_start': 0, 'row_end': 1669}]},\n",
       " {'batch_idx': 163,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 1670, 'row_end': 6679}]},\n",
       " {'batch_idx': 164,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 6680, 'row_end': 11689}]},\n",
       " {'batch_idx': 165,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 11690, 'row_end': 16699}]},\n",
       " {'batch_idx': 166,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 16700, 'row_end': 20373},\n",
       "   {'file_idx': 41, 'row_start': 0, 'row_end': 1335}]},\n",
       " {'batch_idx': 167,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 1336, 'row_end': 6345}]},\n",
       " {'batch_idx': 168,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 6346, 'row_end': 11355}]},\n",
       " {'batch_idx': 169,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 11356, 'row_end': 16365}]},\n",
       " {'batch_idx': 170,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 16366, 'row_end': 20373},\n",
       "   {'file_idx': 42, 'row_start': 0, 'row_end': 1001}]},\n",
       " {'batch_idx': 171,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 1002, 'row_end': 6011}]},\n",
       " {'batch_idx': 172,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 6012, 'row_end': 11021}]},\n",
       " {'batch_idx': 173,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 11022, 'row_end': 16031}]},\n",
       " {'batch_idx': 174,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 16032, 'row_end': 20373},\n",
       "   {'file_idx': 43, 'row_start': 0, 'row_end': 667}]},\n",
       " {'batch_idx': 175,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 668, 'row_end': 5677}]},\n",
       " {'batch_idx': 176,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 5678, 'row_end': 10687}]},\n",
       " {'batch_idx': 177,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 10688, 'row_end': 15697}]},\n",
       " {'batch_idx': 178,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 15698, 'row_end': 20373},\n",
       "   {'file_idx': 44, 'row_start': 0, 'row_end': 333}]},\n",
       " {'batch_idx': 179,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 334, 'row_end': 5343}]},\n",
       " {'batch_idx': 180,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 5344, 'row_end': 10353}]},\n",
       " {'batch_idx': 181,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 10354, 'row_end': 15363}]},\n",
       " {'batch_idx': 182,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 15364, 'row_end': 20373}]},\n",
       " {'batch_idx': 183,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 0, 'row_end': 5009}]},\n",
       " {'batch_idx': 184,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 5010, 'row_end': 10019}]},\n",
       " {'batch_idx': 185,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 10020, 'row_end': 15029}]},\n",
       " {'batch_idx': 186,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 15030, 'row_end': 20039}]},\n",
       " {'batch_idx': 187,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 20040, 'row_end': 20373},\n",
       "   {'file_idx': 46, 'row_start': 0, 'row_end': 4675}]},\n",
       " {'batch_idx': 188,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 4676, 'row_end': 9685}]},\n",
       " {'batch_idx': 189,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 9686, 'row_end': 14695}]},\n",
       " {'batch_idx': 190,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 14696, 'row_end': 19705}]},\n",
       " {'batch_idx': 191,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 19706, 'row_end': 20373},\n",
       "   {'file_idx': 47, 'row_start': 0, 'row_end': 4341}]},\n",
       " {'batch_idx': 192,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 4342, 'row_end': 9351}]},\n",
       " {'batch_idx': 193,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 9352, 'row_end': 14361}]},\n",
       " {'batch_idx': 194,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 14362, 'row_end': 19371}]},\n",
       " {'batch_idx': 195,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 19372, 'row_end': 20373},\n",
       "   {'file_idx': 48, 'row_start': 0, 'row_end': 4007}]},\n",
       " {'batch_idx': 196,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 4008, 'row_end': 9017}]},\n",
       " {'batch_idx': 197,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 9018, 'row_end': 14027}]},\n",
       " {'batch_idx': 198,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 14028, 'row_end': 19037}]},\n",
       " {'batch_idx': 199,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 19038, 'row_end': 20373},\n",
       "   {'file_idx': 49, 'row_start': 0, 'row_end': 3673}]},\n",
       " {'batch_idx': 200,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 3674, 'row_end': 8683}]},\n",
       " {'batch_idx': 201,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 8684, 'row_end': 13693}]},\n",
       " {'batch_idx': 202,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 13694, 'row_end': 18703}]},\n",
       " {'batch_idx': 203,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 18704, 'row_end': 20373},\n",
       "   {'file_idx': 50, 'row_start': 0, 'row_end': 3339}]},\n",
       " {'batch_idx': 204,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 3340, 'row_end': 8349}]},\n",
       " {'batch_idx': 205,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 8350, 'row_end': 13359}]},\n",
       " {'batch_idx': 206,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 13360, 'row_end': 18369}]},\n",
       " {'batch_idx': 207,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 18370, 'row_end': 20373},\n",
       "   {'file_idx': 51, 'row_start': 0, 'row_end': 3005}]},\n",
       " {'batch_idx': 208,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 3006, 'row_end': 8015}]},\n",
       " {'batch_idx': 209,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 8016, 'row_end': 13025}]},\n",
       " {'batch_idx': 210,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 13026, 'row_end': 18035}]},\n",
       " {'batch_idx': 211,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 18036, 'row_end': 20373},\n",
       "   {'file_idx': 52, 'row_start': 0, 'row_end': 2671}]},\n",
       " {'batch_idx': 212,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 2672, 'row_end': 7681}]},\n",
       " {'batch_idx': 213,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 7682, 'row_end': 12691}]},\n",
       " {'batch_idx': 214,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 12692, 'row_end': 17701}]},\n",
       " {'batch_idx': 215,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 17702, 'row_end': 20373},\n",
       "   {'file_idx': 53, 'row_start': 0, 'row_end': 2337}]},\n",
       " {'batch_idx': 216,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 2338, 'row_end': 7347}]},\n",
       " {'batch_idx': 217,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 7348, 'row_end': 12357}]},\n",
       " {'batch_idx': 218,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 12358, 'row_end': 17367}]},\n",
       " {'batch_idx': 219,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 17368, 'row_end': 20373},\n",
       "   {'file_idx': 54, 'row_start': 0, 'row_end': 2003}]},\n",
       " {'batch_idx': 220,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 2004, 'row_end': 7013}]},\n",
       " {'batch_idx': 221,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 7014, 'row_end': 12023}]},\n",
       " {'batch_idx': 222,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 12024, 'row_end': 17033}]},\n",
       " {'batch_idx': 223,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 17034, 'row_end': 20373},\n",
       "   {'file_idx': 55, 'row_start': 0, 'row_end': 1669}]},\n",
       " {'batch_idx': 224,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 1670, 'row_end': 6679}]},\n",
       " {'batch_idx': 225,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 6680, 'row_end': 11689}]},\n",
       " {'batch_idx': 226,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 11690, 'row_end': 16699}]},\n",
       " {'batch_idx': 227,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 16700, 'row_end': 20373},\n",
       "   {'file_idx': 56, 'row_start': 0, 'row_end': 1335}]},\n",
       " {'batch_idx': 228,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 1336, 'row_end': 6345}]},\n",
       " {'batch_idx': 229,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 6346, 'row_end': 11355}]},\n",
       " {'batch_idx': 230,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 11356, 'row_end': 16365}]},\n",
       " {'batch_idx': 231,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 16366, 'row_end': 20373},\n",
       "   {'file_idx': 57, 'row_start': 0, 'row_end': 1001}]},\n",
       " {'batch_idx': 232,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 1002, 'row_end': 6011}]},\n",
       " {'batch_idx': 233,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 6012, 'row_end': 11021}]},\n",
       " {'batch_idx': 234,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 11022, 'row_end': 16031}]},\n",
       " {'batch_idx': 235,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 16032, 'row_end': 20373},\n",
       "   {'file_idx': 58, 'row_start': 0, 'row_end': 667}]},\n",
       " {'batch_idx': 236,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 668, 'row_end': 5677}]},\n",
       " {'batch_idx': 237,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 5678, 'row_end': 10687}]},\n",
       " {'batch_idx': 238,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 10688, 'row_end': 15697}]},\n",
       " {'batch_idx': 239,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 15698, 'row_end': 20373},\n",
       "   {'file_idx': 59, 'row_start': 0, 'row_end': 333}]},\n",
       " {'batch_idx': 240,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 334, 'row_end': 5343}]},\n",
       " {'batch_idx': 241,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 5344, 'row_end': 10353}]},\n",
       " {'batch_idx': 242,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 10354, 'row_end': 15363}]},\n",
       " {'batch_idx': 243,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 15364, 'row_end': 20373}]},\n",
       " {'batch_idx': 244,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 0, 'row_end': 5009}]},\n",
       " {'batch_idx': 245,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 5010, 'row_end': 10019}]},\n",
       " {'batch_idx': 246,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 10020, 'row_end': 15029}]},\n",
       " {'batch_idx': 247,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 15030, 'row_end': 20039}]},\n",
       " {'batch_idx': 248,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 20040, 'row_end': 20373},\n",
       "   {'file_idx': 61, 'row_start': 0, 'row_end': 4675}]},\n",
       " {'batch_idx': 249,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 4676, 'row_end': 9685}]},\n",
       " {'batch_idx': 250,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 9686, 'row_end': 14695}]},\n",
       " {'batch_idx': 251,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 14696, 'row_end': 19705}]},\n",
       " {'batch_idx': 252,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 19706, 'row_end': 20373},\n",
       "   {'file_idx': 62, 'row_start': 0, 'row_end': 4341}]},\n",
       " {'batch_idx': 253,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 4342, 'row_end': 9351}]},\n",
       " {'batch_idx': 254,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 9352, 'row_end': 14361}]},\n",
       " {'batch_idx': 255,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 14362, 'row_end': 19371}]},\n",
       " {'batch_idx': 256,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 19372, 'row_end': 20373},\n",
       "   {'file_idx': 63, 'row_start': 0, 'row_end': 4007}]},\n",
       " {'batch_idx': 257,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 4008, 'row_end': 9017}]},\n",
       " {'batch_idx': 258,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 9018, 'row_end': 14027}]},\n",
       " {'batch_idx': 259,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 14028, 'row_end': 19037}]},\n",
       " {'batch_idx': 260,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 19038, 'row_end': 20373},\n",
       "   {'file_idx': 64, 'row_start': 0, 'row_end': 3673}]},\n",
       " {'batch_idx': 261,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 3674, 'row_end': 8683}]},\n",
       " {'batch_idx': 262,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 8684, 'row_end': 13693}]},\n",
       " {'batch_idx': 263,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 13694, 'row_end': 18703}]},\n",
       " {'batch_idx': 264,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 18704, 'row_end': 20373},\n",
       "   {'file_idx': 65, 'row_start': 0, 'row_end': 3339}]},\n",
       " {'batch_idx': 265,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 3340, 'row_end': 8349}]},\n",
       " {'batch_idx': 266,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 8350, 'row_end': 13359}]},\n",
       " {'batch_idx': 267,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 13360, 'row_end': 18369}]},\n",
       " {'batch_idx': 268,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 18370, 'row_end': 20373},\n",
       "   {'file_idx': 66, 'row_start': 0, 'row_end': 3005}]},\n",
       " {'batch_idx': 269,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 3006, 'row_end': 8015}]},\n",
       " {'batch_idx': 270,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 8016, 'row_end': 13025}]},\n",
       " {'batch_idx': 271,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 13026, 'row_end': 18035}]},\n",
       " {'batch_idx': 272,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 18036, 'row_end': 20373},\n",
       "   {'file_idx': 67, 'row_start': 0, 'row_end': 2671}]},\n",
       " {'batch_idx': 273,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 2672, 'row_end': 7681}]},\n",
       " {'batch_idx': 274,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 7682, 'row_end': 12691}]},\n",
       " {'batch_idx': 275,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 12692, 'row_end': 17701}]},\n",
       " {'batch_idx': 276,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 17702, 'row_end': 20373},\n",
       "   {'file_idx': 68, 'row_start': 0, 'row_end': 2337}]},\n",
       " {'batch_idx': 277,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 2338, 'row_end': 7347}]},\n",
       " {'batch_idx': 278,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 7348, 'row_end': 12357}]},\n",
       " {'batch_idx': 279,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 12358, 'row_end': 17367}]},\n",
       " {'batch_idx': 280,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 17368, 'row_end': 20373},\n",
       "   {'file_idx': 69, 'row_start': 0, 'row_end': 2003}]},\n",
       " {'batch_idx': 281,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 2004, 'row_end': 7013}]},\n",
       " {'batch_idx': 282,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 7014, 'row_end': 12023}]},\n",
       " {'batch_idx': 283,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 12024, 'row_end': 17033}]},\n",
       " {'batch_idx': 284,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 17034, 'row_end': 20373},\n",
       "   {'file_idx': 70, 'row_start': 0, 'row_end': 1669}]},\n",
       " {'batch_idx': 285,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 1670, 'row_end': 6679}]},\n",
       " {'batch_idx': 286,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 6680, 'row_end': 11689}]},\n",
       " {'batch_idx': 287,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 11690, 'row_end': 16699}]},\n",
       " {'batch_idx': 288,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 16700, 'row_end': 20373},\n",
       "   {'file_idx': 71, 'row_start': 0, 'row_end': 1335}]},\n",
       " {'batch_idx': 289,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 1336, 'row_end': 6345}]},\n",
       " {'batch_idx': 290,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 6346, 'row_end': 11355}]},\n",
       " {'batch_idx': 291,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 11356, 'row_end': 16365}]},\n",
       " {'batch_idx': 292,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 16366, 'row_end': 20373},\n",
       "   {'file_idx': 72, 'row_start': 0, 'row_end': 1001}]},\n",
       " {'batch_idx': 293,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 1002, 'row_end': 6011}]},\n",
       " {'batch_idx': 294,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 6012, 'row_end': 11021}]},\n",
       " {'batch_idx': 295,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 11022, 'row_end': 16031}]},\n",
       " {'batch_idx': 296,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 16032, 'row_end': 20373},\n",
       "   {'file_idx': 73, 'row_start': 0, 'row_end': 667}]},\n",
       " {'batch_idx': 297,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 668, 'row_end': 5677}]},\n",
       " {'batch_idx': 298,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 5678, 'row_end': 10687}]},\n",
       " {'batch_idx': 299,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 10688, 'row_end': 15697}]},\n",
       " {'batch_idx': 300,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 15698, 'row_end': 20373},\n",
       "   {'file_idx': 74, 'row_start': 0, 'row_end': 333}]},\n",
       " {'batch_idx': 301,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 334, 'row_end': 5343}]},\n",
       " {'batch_idx': 302,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 5344, 'row_end': 10353}]},\n",
       " {'batch_idx': 303,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 10354, 'row_end': 15363}]},\n",
       " {'batch_idx': 304,\n",
       "  'target_batch_size': 5010,\n",
       "  'actual_batch_size': 5010,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 15364, 'row_end': 20373}]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.batch_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n",
      "5010\n"
     ]
    }
   ],
   "source": [
    "for bm in training_generator.batch_metadata:\n",
    "    print(bm['actual_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228c7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building LUT: 100%|| 306/306 [00:19<00:00, 15.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# def build_vec_index(gen):\n",
    "#     lbls, batches, events = [], [], []\n",
    "#     for b in tqdm(range(len(gen)), desc = \"Building LUT\"):\n",
    "#         _, yb = gen[b]\n",
    "#         # arr = np.round(yb.numpy() * SCALE).astype(\"int64\")\n",
    "#         arr = yb.numpy()\n",
    "#         lbls.append(arr)\n",
    "#         batches.append(np.full(len(arr), b, \"int32\"))\n",
    "#         events.append(np.arange(len(arr), dtype=\"int32\"))\n",
    "#     lbls   = np.vstack(lbls)               # shape (N, 4)\n",
    "#     batches = np.concatenate(batches)\n",
    "#     events  = np.concatenate(events)\n",
    "#     return lbls, batches, events           # all NumPy\n",
    "\n",
    "# lbls,bs,es    = build_vec_index(training_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0553d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cef1b-61db-489f-83bb-039a46861094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 13, 21, 2)]       0         \n",
      "                                                                 \n",
      " q_separable_conv2d_1 (QSep  (None, 11, 19, 5)         33        \n",
      " arableConv2D)                                                   \n",
      "                                                                 \n",
      " q_activation_5 (QActivatio  (None, 11, 19, 5)         0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_conv2d_1 (QConv2D)        (None, 11, 19, 5)         30        \n",
      "                                                                 \n",
      " q_activation_6 (QActivatio  (None, 11, 19, 5)         0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 3, 6, 5)           0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " q_activation_7 (QActivatio  (None, 3, 6, 5)           0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 90)                0         \n",
      "                                                                 \n",
      " q_dense_3 (QDense)          (None, 16)                1456      \n",
      "                                                                 \n",
      " q_activation_8 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_4 (QDense)          (None, 16)                272       \n",
      "                                                                 \n",
      " q_activation_9 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_5 (QDense)          (None, 14)                238       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2029 (7.93 KB)\n",
      "Trainable params: 2029 (7.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CreateModel((13,21,2),n_filters=5,pool_size=3)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce60b630-5e1e-41cd-b9a0-7e9a06e9a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "base_dir = f'./trained_models/model-{fingerprint}-checkpoints'\n",
    "os.makedirs(base_dir, exist_ok=True)  \n",
    "checkpoint_filepath = base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ccc68b0-0d5a-4b14-bebc-0d8e6923e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e0ef8b33\n"
     ]
    }
   ],
   "source": [
    "print(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd97cbbf-a137-4a5c-9796-11de00abfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "early_stopping_patience = 50\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        checkpoints = [f for f in os.listdir(base_dir) if f.startswith('weights')]\n",
    "        if len(checkpoints) > 1:\n",
    "            checkpoints.sort()\n",
    "            for checkpoint in checkpoints[:-1]:\n",
    "                os.remove(os.path.join(base_dir, checkpoint))\n",
    "\n",
    "es = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "mcp = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(f'{base_dir}/training_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "305/305 [==============================] - ETA: 0s - loss: 27173.7227\n",
      "Epoch 1: val_loss improved from inf to 3957.69629, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.01-t27173.72-v3957.70.hdf5\n",
      "305/305 [==============================] - 34s 102ms/step - loss: 27173.7227 - val_loss: 3957.6963\n",
      "Epoch 2/1000\n",
      "305/305 [==============================] - ETA: 0s - loss: 2077.7952\n",
      "Epoch 2: val_loss improved from 3957.69629 to -473.36844, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.02-t2077.80-v-473.37.hdf5\n",
      "305/305 [==============================] - 30s 97ms/step - loss: 2077.7952 - val_loss: -473.3684\n",
      "Epoch 3/1000\n",
      "305/305 [==============================] - ETA: 0s - loss: -935.6221\n",
      "Epoch 3: val_loss improved from -473.36844 to -1284.97668, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.03-t-935.62-v-1284.98.hdf5\n",
      "305/305 [==============================] - 61s 199ms/step - loss: -935.6221 - val_loss: -1284.9767\n",
      "Epoch 4/1000\n",
      "305/305 [==============================] - ETA: 0s - loss: -484.1406\n",
      "Epoch 4: val_loss improved from -1284.97668 to -1554.20630, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.04-t-484.14-v-1554.21.hdf5\n",
      "305/305 [==============================] - 36s 116ms/step - loss: -484.1406 - val_loss: -1554.2063\n",
      "Epoch 5/1000\n",
      "305/305 [==============================] - ETA: 0s - loss: -3135.4961\n",
      "Epoch 5: val_loss improved from -1554.20630 to -2936.62402, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.05-t-3135.50-v-2936.62.hdf5\n",
      "305/305 [==============================] - 40s 130ms/step - loss: -3135.4961 - val_loss: -2936.6240\n",
      "Epoch 6/1000\n",
      " 38/305 [==>...........................] - ETA: 19s - loss: -3885.3333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/depot/cms/kernels/python3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        x=training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[es, mcp, csv_logger],\n",
    "        epochs=1000,\n",
    "        shuffle=False,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e2706-3e27-4ece-9ebd-be135f6bbcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
